{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'inter_table1.csv' saved successfully.\n",
      "An error occurred while fetching tables: 'Table_2'\n",
      "No Important rows found!!!\n",
      "CSV file 'usa_table1.csv' saved successfully.\n",
      "An error occurred while fetching tables: 'Table_2'\n",
      "No Important rows found!!!\n",
      "CSV file 'inter_table1_filtered.csv' saved successfully.\n",
      "An error occurred while fetching tables: 'Table_2'\n",
      "No Important rows found!!!\n",
      "CSV file 'usa_table1_filtered.csv' saved successfully.\n",
      "An error occurred while fetching tables: 'Table_2'\n",
      "No Important rows found!!!\n",
      "Data extraction and processing complete.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "def setup_driver(driver_path):\n",
    "    \"\"\"\n",
    "    Set up the Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    service = Service(executable_path=driver_path)\n",
    "    return webdriver.Chrome(service=service)\n",
    "\n",
    "def wait_and_click(driver, by, value, timeout=10):\n",
    "    \"\"\"\n",
    "    Wait for an element to be clickable and click it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((by, value))\n",
    "        )\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Element not found or could not be clicked: {str(e)}\")\n",
    "\n",
    "def get_select_elements(driver, by, value, timeout=10):\n",
    "    \"\"\"\n",
    "    Wait for and return all elements matching the selector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        elements = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_all_elements_located((by, value))\n",
    "        )\n",
    "        return elements\n",
    "    except Exception as e:\n",
    "        print(f\"Elements not found: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_designs(designs_list, available_designs):\n",
    "    \"\"\"\n",
    "    Filter and return the selected designs from the available designs list.\n",
    "    \"\"\"\n",
    "    selected_designs = []\n",
    "    for design in available_designs:\n",
    "        if design.text in designs_list:\n",
    "            selected_designs.append(design)\n",
    "    return selected_designs\n",
    "\n",
    "def collect_table_info(driver, div_class):\n",
    "    \"\"\"\n",
    "    Collect both table headers and data based on the specified div class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_div = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, f'//div[@class=\"{div_class} \"]'))\n",
    "        )\n",
    "        tables = data_div.find_elements(By.XPATH, './/table[@id=\"tblResultView\"]')\n",
    "        if div_class == 'data_in_inch':\n",
    "            driver.execute_script(\"arguments[0].style.display = 'block';\", data_div)\n",
    "        \n",
    "        table_info = {}\n",
    "        for i, table in enumerate(tables):\n",
    "            header_rows = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'tr')\n",
    "            headers_dict = {index: row.find_elements(By.TAG_NAME, 'th') for index, row in enumerate(header_rows) if index != 3}\n",
    "            headers_text = {\n",
    "                index: {i: th.text for i, th in enumerate(th_list)}\n",
    "                for index, th_list in headers_dict.items()\n",
    "            }\n",
    "            \n",
    "            table_data_rows = table.find_elements(By.TAG_NAME, 'tbody')[0].find_elements(By.TAG_NAME, 'tr')\n",
    "            data_rows = [{i: td.text for i, td in enumerate(row.find_elements(By.TAG_NAME, 'td'))} for row in table_data_rows]\n",
    "            \n",
    "            table_info[f\"Table_{i+1}\"] = {\n",
    "                    \"headers\": headers_text,\n",
    "                    \"data\": data_rows\n",
    "                }\n",
    "        \n",
    "        return table_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching tables: {str(e)}\")\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def collect_filtered_data(driver, div_class):\n",
    "    \"\"\"\n",
    "    Collect filtered data from tables based on the specified div class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_div = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, f'//div[@class=\"{div_class} \"]'))\n",
    "        )\n",
    "        tables = data_div.find_elements(By.XPATH, './/table[@id=\"tblResultView\"]')\n",
    "        if div_class == 'data_in_inch':\n",
    "            driver.execute_script(\"arguments[0].style.display = 'block';\", data_div)\n",
    "        \n",
    "        table_data = []\n",
    "        \n",
    "        xpath_expression = f'//tbody/tr[./td[@style=\"color:#a30e13 !important\"]]'\n",
    "        table_unit = {'data_in_mm': 'tables_mm', 'data_in_inch': 'tables_inch'}\n",
    "\n",
    "        if table_unit[div_class] == 'tables_mm':\n",
    "\n",
    "            table_info = {}\n",
    "\n",
    "            for i, table in enumerate(tables):\n",
    "\n",
    "                header_rows = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'tr')\n",
    "                headers_dict = {index: row.find_elements(By.TAG_NAME, 'th') for index, row in enumerate(header_rows) if index != 3}\n",
    "                headers_text = {\n",
    "                    index: {i: th.text for i, th in enumerate(th_list)}\n",
    "                    for index, th_list in headers_dict.items()\n",
    "                }\n",
    "\n",
    "                table_data_in_mm = table.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "                data_rows = [{i: td.text for i, td in enumerate(table_data_in_mm[0].find_elements(By.TAG_NAME, 'td'))}]\n",
    "                data_row = {k: data_rows[0][k] for k in range(7)}\n",
    "\n",
    "                filtered_data = {}\n",
    "                for index, tr in enumerate(table_data_in_mm):\n",
    "                    filter_list = tr.find_elements(By.XPATH, xpath_expression)\n",
    "                    for idx, ftr in enumerate(filter_list):\n",
    "                        filtered_data[idx] = ftr.text\n",
    "\n",
    "                data = filtered_data[i]\n",
    "                table_data.append({0: data})\n",
    "                \n",
    "                processed_data = [\n",
    "                    {\n",
    "                        i: {j: txt for j, txt in enumerate(td.split(' '))}\n",
    "                        for i, td in table.items()\n",
    "                    }\n",
    "                    for table in table_data\n",
    "                ]\n",
    "\n",
    "                table_info[f\"Table_{i+1}\"] = {\n",
    "                    \"headers\": headers_text,\n",
    "                    \"data\": [data_row, processed_data[i]]\n",
    "                }\n",
    "\n",
    "            return table_info\n",
    "\n",
    "        else:\n",
    "            table_info = {}\n",
    "\n",
    "            for i, table in enumerate(tables):\n",
    "\n",
    "                header_rows = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'tr')\n",
    "                headers_dict = {index: row.find_elements(By.TAG_NAME, 'th') for index, row in enumerate(header_rows) if index != 3}\n",
    "                headers_text = {\n",
    "                    index: {i: th.text for i, th in enumerate(th_list)}\n",
    "                    for index, th_list in headers_dict.items()\n",
    "                }\n",
    "\n",
    "                table_data_in_inch = table.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "                data_rows = [{i: td.text for i, td in enumerate(table_data_in_inch[0].find_elements(By.TAG_NAME, 'td'))}]\n",
    "                data_row = {k: data_rows[0][k] for k in range(7)}\n",
    "\n",
    "                filtered_data = {}\n",
    "                for index, tr in enumerate(table_data_in_inch, start=2):\n",
    "                    filter_list = tr.find_elements(By.XPATH, xpath_expression)\n",
    "                    for idx, ftr in enumerate(filter_list):\n",
    "                        filtered_data[idx] = ftr.text\n",
    "\n",
    "                data = filtered_data[i+2]\n",
    "                table_data.append({0: data})\n",
    "\n",
    "                processed_data = [\n",
    "                    {\n",
    "                        i: {j: txt for j, txt in enumerate(td.split(' '))}\n",
    "                        for i, td in table.items()\n",
    "                    }\n",
    "                    for table in table_data\n",
    "                ]\n",
    "\n",
    "                table_info[f\"Table_{i+1}\"] = {\n",
    "                    \"headers\": headers_text,\n",
    "                    \"data\": [data_row, processed_data[i]]\n",
    "                }\n",
    "        \n",
    "        return table_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching tables: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_headers(table):\n",
    "    \"\"\"\n",
    "    Extracts and processes the headers from the table.\n",
    "    \"\"\"\n",
    "    row1 = [text.replace('\\n', ' ') for text in list(table[0].values())]\n",
    "    row2 = list(table[1].values())\n",
    "    row3 = list(table[2].values())\n",
    "    row4 = [text.replace('\\n', ' ') for text in list(table[4].values())]\n",
    "    \n",
    "    return row1, row2, row3, row4\n",
    "\n",
    "def create_multi_index(row1, row2, row3, row4):\n",
    "    \"\"\"\n",
    "    Creates a multi-level index for DataFrame columns based on the headers.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "\n",
    "    for col in row1:\n",
    "        if col == 'Unloaded Dimension':\n",
    "            if row4:\n",
    "                tuples.extend([(col, '', '', row4[0]), (col, '', '', row4[1])])\n",
    "            else:\n",
    "                tuples.append((col, '', '', ''))\n",
    "        elif col == 'Recommended Load':\n",
    "            if row2 and row3 and row4:\n",
    "                tuples.extend([(col, row2[0], row3[0], subcol) for subcol in row4[2:]])\n",
    "            elif row2 and row4:\n",
    "                tuples.extend([(col, row2[0], '', subcol) for subcol in row4[2:]])\n",
    "            elif row4:\n",
    "                tuples.extend([(col, '', '', subcol) for subcol in row4[2:]])\n",
    "        else:\n",
    "            tuples.append((col, '', '', ''))\n",
    "    \n",
    "    return pd.MultiIndex.from_tuples(tuples)\n",
    "\n",
    "def create_dataframe(headers, sample_data):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame from the processed headers and sample data.\n",
    "    \"\"\"\n",
    "    row1, row2, row3, row4 = extract_headers(headers)\n",
    "    columns = create_multi_index(row1, row2, row3, row4)\n",
    "    return pd.DataFrame(sample_data, columns=columns)\n",
    "\n",
    "def process_table(table_info, table_key):\n",
    "    try:\n",
    "        columns = table_info[table_key]['headers']\n",
    "        table_data = table_info[table_key]['data']\n",
    "\n",
    "        dr1 = [text.replace('\\n', ' ') for text in list(table_data[0].values())]\n",
    "\n",
    "        if len(dr1) <= 7:\n",
    "            max_key = max(table_data[0])\n",
    "            combined_dict = table_data[0].copy()\n",
    "\n",
    "            for k,v in table_data[1][0].items():\n",
    "                combined_dict[max_key + 1 + k] = v\n",
    "\n",
    "            combined_data = list(combined_dict.values())\n",
    "\n",
    "            data_rows = [combined_data]\n",
    "\n",
    "            return create_dataframe(columns, data_rows)\n",
    "        \n",
    "        else:\n",
    "            data_rows = [dr1]\n",
    "\n",
    "            max_length = max(len(list(dr.values())) for dr in table_data)\n",
    "\n",
    "            for i, dr in enumerate(table_data[1:], start=2):\n",
    "                padding_length = max_length - len(list(dr.values()))\n",
    "                drs = [''] * padding_length + list(dr.values())\n",
    "                data_rows.append(drs)\n",
    "\n",
    "        return create_dataframe(columns, data_rows)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching tables: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(df, folder, filename):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(os.path.join(folder,filename), index=False)\n",
    "        print(f\"CSV file '{filename}' saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"No Important rows found!!!\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    website = \"https://yokohama-atg.com/usa/tire-selector-yokohama-off-highway-tires/\"\n",
    "    driver_path = \"C:\\\\Users\\\\cheta\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe\"  # Change according to your PC's file path\n",
    "    # my_designs = '350'\n",
    "    my_designs = input(\"Enter the design: \")\n",
    "    folder = 'Alliance'\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder) \n",
    "\n",
    "    subfolder_path = os.path.join(folder, my_designs)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.mkdir(subfolder_path)\n",
    "\n",
    "    try:\n",
    "        # Initialize WebDriver\n",
    "        driver = setup_driver(driver_path)\n",
    "        driver.get(website)\n",
    "\n",
    "        # Accept cookies\n",
    "        wait_and_click(driver, By.XPATH, '//span[@data-cookie-set=\"accept\"]')\n",
    "\n",
    "        # Select brand\n",
    "        select_buttons = get_select_elements(driver, By.XPATH, '//span[@class=\"select2-arrow\"]')\n",
    "        brands_button = select_buttons[2]\n",
    "        brands_button.click()\n",
    "        brands = get_select_elements(driver, By.XPATH, '//li[@class=\"select2-results-dept-0 select2-result select2-result-selectable\"]')\n",
    "        brands[0].click()\n",
    "\n",
    "        # Wait for the loader to disappear\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element((By.XPATH, '//div[@class=\"loader-wrapper\"]')))\n",
    "\n",
    "        # Select design\n",
    "        design_button = select_buttons[3]\n",
    "        design_button.click()\n",
    "        available_designs = get_select_elements(driver, By.XPATH, '//li[@class=\"select2-results-dept-0 select2-result select2-result-selectable\"]')\n",
    "        selected_designs = get_designs([my_designs], available_designs)\n",
    "        if selected_designs:\n",
    "            selected_designs[0].click()\n",
    "\n",
    "        wait_and_click(driver, By.XPATH, '//a[@title=\"Search\"]')\n",
    "\n",
    "        international_data = collect_table_info(driver, \"data_in_mm\")\n",
    "        usa_data = collect_table_info(driver, \"data_in_inch\")\n",
    "\n",
    "        international_filtered_data = collect_filtered_data(driver, \"data_in_mm\")\n",
    "        usa_filtered_data = collect_filtered_data(driver, \"data_in_inch\")\n",
    "\n",
    "        inter_table1 = process_table(international_data, 'Table_1')\n",
    "        save_to_csv(inter_table1, subfolder_path, 'inter_table1.csv')\n",
    "\n",
    "        inter_table2 = process_table(international_data, 'Table_2')\n",
    "        save_to_csv(inter_table2, subfolder_path, 'inter_table2.csv')\n",
    "\n",
    "        usa_table1 = process_table(usa_data, 'Table_1')\n",
    "        save_to_csv(usa_table1, subfolder_path, 'usa_table1.csv')\n",
    "\n",
    "        usa_table2 = process_table(usa_data, 'Table_2')\n",
    "        save_to_csv(usa_table2, subfolder_path, 'usa_table2.csv')\n",
    "\n",
    "        inter_table1_filtered = process_table(international_filtered_data, 'Table_1')\n",
    "        save_to_csv(inter_table1_filtered, subfolder_path, 'inter_table1_filtered.csv')\n",
    "\n",
    "        inter_table2_filtered  = process_table(international_filtered_data, 'Table_2')\n",
    "        save_to_csv(inter_table2_filtered, subfolder_path, 'inter_table2_filtered.csv')\n",
    "\n",
    "        usa_table1_filtered  = process_table(usa_filtered_data, 'Table_1')\n",
    "        save_to_csv(usa_table1_filtered, subfolder_path, 'usa_table1_filtered.csv')\n",
    "\n",
    "        usa_table2_filtered  = process_table(usa_filtered_data, 'Table_2')\n",
    "        save_to_csv(usa_table2_filtered, subfolder_path, 'usa_table2_filtered.csv')\n",
    "\n",
    "    finally:\n",
    "        time.sleep(5)\n",
    "        driver.quit()\n",
    "        print(\"Data extraction and processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
